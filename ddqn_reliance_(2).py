# -*- coding: utf-8 -*-
"""ddqn_reliance (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f3BThqW7muAv9atykMhiVYA4zbeV_bGz
"""

!pip install yfinance
!pip install ta

# ignore warnings
import warnings
warnings.filterwarnings('ignore')

"""Load Data"""

import yfinance as yf
import datetime

# Define the stock ticker symbol and the time period
## DO NOT CHANE THESE VALUES!
stock_symbol = 'RELIANCE.NS'
start_date = '2014-10-04'
end_date = '2024-10-04'

# Fetch the historical stock price data
stock_data = yf.download(stock_symbol, start=start_date, end=end_date)

import plotly
import plotly.graph_objs as go

# Create a line plot of the stock's closing price-
fig = go.Figure(data=[go.Scatter(x=stock_data.index, y=stock_data['Close'], mode='lines', name='Close Price')])

# Set the layout for the plot
fig.update_layout(title=f'Stock Price of {stock_symbol}', xaxis_title='Date', yaxis_title='Price (USD)', template='plotly_dark')

# Display the plot
fig.show()

"""Technical Index"""

import ta

# RSI
stock_data['RSI'] = ta.momentum.rsi(stock_data['Close'], window=14, fillna=True)

# SMA
stock_data['SMA'] = ta.trend.sma_indicator(stock_data['Close'], window=12, fillna=True)

# OBV
stock_data['OBV'] = ta.volume.on_balance_volume(stock_data['Close'], stock_data['Volume'], fillna=True)

# MACD related
# create an macd object
macd = ta.trend.MACD(stock_data['Close'], window_slow=26, window_fast=12, window_sign=9, fillna=True)

# MACD line
stock_data['MACD'] = macd.macd()

# MACD signal line
stock_data['MACD_signal'] = macd.macd_signal()

# MACD histogram/diff
stock_data['MACD_diff'] = macd.macd_diff()

# Calculate daily price difference
stock_data['Price_diff'] = stock_data['Close'].diff().fillna(0)

stock_data.head()

stock_data.info()

"""Train-Test Split"""

from sklearn.model_selection import train_test_split

df_train, df_test = train_test_split(stock_data, test_size=0.3, shuffle=False)

df_test.info()

sequence_length = df_train['2014-10-04':'2021-10-04'].shape[0]
sequence_length

df_train.to_csv('RELIANCE.NS_stock.csv', index=False)

"""EDA"""

df_train.describe()

import matplotlib.pyplot as plt
import seaborn as sns

# Set the style of seaborn
sns.set_style('whitegrid')

# Plot the distribution of 'Close' prices
plt.figure(figsize=(10, 6))
sns.histplot(df_train['Close'], bins=50, kde=True)
plt.title('Distribution of Close Prices')
plt.xlabel('Close Price')
plt.ylabel('Frequency')

# Show the plot
plt.show()

# Calculate the median and variance of 'Close' prices
close_std = df_train['Close'].std()
close_std

"""Correlation Analysis"""

# Compute the correlation matrix
correlation_matrix = df_train.corr()

# Plot correlation matrix
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.show()

# standardize the data
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
features_to_scale = [col for col in df_train.columns if col not in ['Close']]
df_train_scaled = scaler.fit_transform(df_train[features_to_scale])
df_test_scaled = scaler.transform(df_test[features_to_scale])

df_train.head()

import pandas as pd

df_train_scaled = pd.DataFrame(df_train_scaled, columns=features_to_scale, index=df_train.index)
df_test_scaled = pd.DataFrame(df_test_scaled, columns=features_to_scale, index=df_test.index)

df_train_scaled['Close'] = df_train['Close']
df_test_scaled['Close'] = df_test['Close']

df_train_scaled.head()

"""Deep Reinforcement Learning (Implementing DDQN)

Change from DQM to DDQN is to implement 2 different Q-values in the agent (action - main Q-value) and evaluation (target Q-value)

Environment:
"""

import gym
from gym import spaces
import numpy as np
import pandas as pd

class StockTradingEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, df, initial_balance=50000, loss_limit=-4000, window=70):
        super().__init__()
        self.df = df
        self.initial_balance = initial_balance
        self.loss_limit = loss_limit
        self.window = window  # historical price window
        self.action_space = spaces.Discrete(3)  # 0: Hold, 1: Buy 1 stock, 2: Sell all

    # Reset the environment to the initial state
    def reset(self):
        self.current_step = self.window
        self.done = False
        self.total_profit = 0
        self.balance = self.initial_balance # available balance
        self.positions = []  # store positions
        self.history = self.df.iloc[self.current_step-self.window:self.current_step].values # initial history
        return self.get_observation()  # get_observation for the initial state

    def get_observation(self):
        stock_info = self.history
        extra_info = np.repeat([[self.total_profit, self.balance, sum(self.positions)]], self.window, axis=0)
        return np.concatenate([stock_info, extra_info], axis=1) # Concatenate along the features axis

    # Get quintuple components
    def step(self, action):

        self.current_price = self.df.iloc[self.current_step-1]['Close']
        reward = 0  # Initialize reward

        if action == 1:  # buy
            self.balance -= self.current_price # adjust balance
            self.positions.append(self.current_price) # buy one stock at current price

        elif action == 2:  # sell
            if self.positions:
                sold_amount = sum(self.positions)
                gains = self.current_price*len(self.positions) - sold_amount
                self.balance += self.current_price*len(self.positions) # adjust balance
                self.total_profit += gains
                reward = gains
                self.positions = [] # clear positions
            else:
                reward = -10  # negative reward for invalid sell with no stocks

        self.current_step += 1
        # Check if the episode is done
        if self.total_profit <= self.loss_limit or self.current_step >= self.df.shape[0]:
            self.done = True

        self.history = np.array(self.df.iloc[self.current_step-self.window:self.current_step]) # update history
        next_state = self.get_observation() # Get observation of the next step

        return next_state, reward, self.done, {'Trade price': self.current_price , 'Total profit': self.total_profit, 'Available balance': self.balance, 'Position value': sum(self.positions)}

    # Render the environment
    def render(self, mode='human', close=False):
        action = self.action_space.sample() # choose a random action
        next_state, reward, done, info = self.step(action)
        print(f'Step:{self.current_step}')
        print(f'Next State: {next_state}')
        print(f"Action:{action}, Reward:{reward}, Done:{done}, Info:{info}")

# keep only first 5 rows of df_train
data = df_train_scaled.iloc[:15][['Close', 'RSI', 'SMA', 'MACD']]
data

"""Testing Environmnet"""

# Create the environment
env = StockTradingEnv(data, window=3)

# Check initial state
initial_state = env.reset()
initial_state

# Test the environment
observation = env.reset()
for i in range(12):
    env.render()
    print()

# Close the environment
env.close()

"""Model (Check once whether this is suitable for ddqn)"""

import torch
import torch.nn as nn
import torch.optim as optim
import random
from collections import deque

# Define the DQN model with LSTM
class DQN_lstm(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=1):
        super(DQN_lstm, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)  # Final fully connected layer

    def forward(self, x):
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)  # Initial hidden state
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(device)  # Initial cell state
        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_dim)
        out = out[:, -1, :]  # Get outputs for the last time step
        return self.fc(out)

"""Replay Memory"""

# Define the Replay Memory
class ReplayMemory:
    def __init__(self, capacity):
        self.capacity = capacity # maximum
        self.memory = deque(maxlen=capacity) # replace the oldest transition, represented in the form of quintuple, with the latest transit when exceeds the maximum capacity

    def push(self, quintuple): # push a new transition to the memory
        self.memory.append(quintuple)

    def sample(self, batch_size): # random sample transitions of batch_size from the memory and use for training
        return random.sample(self.memory, batch_size)

    def __len__(self):
        return len(self.memory) # return the current number of transitions in the memory

"""Agent"""

class DDQNAgent:
    def __init__(self, state_size, action_size, policy_net, target_net, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=200):
        self.state_size = state_size
        self.action_size = action_size
        self.epsilon = epsilon_start  # Epsilon-greedy policy to ensure exploration
        self.epsilon_min = epsilon_end  # Minimum epsilon value
        self.epsilon_decay = epsilon_decay  # Control decay rate
        self.policy_net = policy_net  # Main Q-network
        self.target_net = target_net  # Target Q-network
        self.target_net.eval()  # Target network is only used for evaluation

    def act(self, state, test=False):
        # Epsilon-greedy action selection
        if not test and random.random() <= self.epsilon:
            return random.randrange(self.action_size)  # Random action (exploration)
        else:
            state = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                action_values = self.policy_net(state)  # Use policy_net for action selection
            return np.argmax(action_values.cpu().detach().numpy())  # Choose the best action

    def update_epsilon(self):
        # Decay epsilon after each episode
        self.epsilon = max(self.epsilon_min, self.epsilon * (1 - 1/self.epsilon_decay))

    def update_target_network(self):
        # Update the target_net to match the policy_net
        self.target_net.load_state_dict(self.policy_net.state_dict())

"""Trainer"""

class DDQNTrainer:
    def __init__(self, agent, memory, batch_size=128, gamma=0.99, lr=1e-3):
        self.agent = agent
        self.memory = memory
        self.batch_size = batch_size
        self.gamma = gamma  # Discount factor for future rewards
        self.optimizer = optim.Adam(agent.policy_net.parameters(), lr=lr)  # Use policy_net for optimization

    def train_step(self):
        # Sample a minibatch from memory
        minibatch = self.memory.sample(self.batch_size)
        states, actions, rewards, next_states, dones = zip(*minibatch)

        # Convert minibatch to tensors
        states = torch.stack([torch.FloatTensor(s) for s in states]).to(device)
        next_states = torch.stack([torch.FloatTensor(s) for s in next_states]).to(device)
        actions = torch.LongTensor(np.array(actions)).view(-1, 1).to(device)
        rewards = torch.FloatTensor(np.array(rewards)).to(device)
        dones = torch.FloatTensor(np.array(dones)).to(device)

        # Compute Q_expected using the policy network
        Q_expected = self.agent.policy_net(states).gather(1, actions).squeeze()

        # Compute Q_target using DDQN approach
        with torch.no_grad():
            # Use policy_net to select the best action in the next state
            next_actions = self.agent.policy_net(next_states).argmax(dim=1, keepdim=True)

            # Use target_net to compute the Q-value of the selected action
            Q_next = self.agent.target_net(next_states).gather(1, next_actions).squeeze()
            Q_target = rewards + (self.gamma * Q_next * (1 - dones))

        # Compute the loss between Q_expected and Q_target
        loss = nn.MSELoss()(Q_expected, Q_target)

        # Backpropagation
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return loss.item()

"""LSTM"""

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
window_size = 20
state_size = df_train_scaled.shape[1] + 3
action_size = env.action_space.n

# Initialize environment
env = StockTradingEnv(df_train_scaled, window=window_size)

# Initialize the model
lstm_policy = DQN_lstm(input_dim=state_size, hidden_dim=64, output_dim=action_size).to(device)
lstm_target = DQN_lstm(input_dim=state_size, hidden_dim=64, output_dim=action_size).to(device)

# Initialize agent
agent = DDQNAgent(state_size=state_size, action_size=action_size, policy_net=lstm_policy, target_net=lstm_target)

# Initialize replay memory
memory = ReplayMemory(capacity=10000)

# Initialize trainer
trainer = DDQNTrainer(agent, memory)

episode_reward = []
episode_loss = []
num_episodes = 100
initial_memory = 7000

# Fill up the memory
while len(memory) < initial_memory:
    state = env.reset()
    done = False

    while not done:
        action = agent.act(state)
        next_state, reward, done, _ = env.step(action)
        memory.push((state, action, reward, next_state, done))
        state = next_state

for episode in range(num_episodes):
    lstm_policy.train()  # Train policy network
    state = env.reset()
    total_reward = 0
    total_loss = []
    done = False

    while not done:
        action = agent.act(state)
        next_state, reward, done, info = env.step(action)
        memory.push((state, action, reward, next_state, done))

        # Train the model and get the loss
        loss = trainer.train_step()
        total_loss.append(loss)

        # Update epsilon for exploration
        agent.update_epsilon()

        state = next_state
        total_reward += reward

    # Update target network at fixed intervals, e.g., every 10 episodes
    if (episode + 1) % 10 == 0:
        agent.update_target_network()  # Update target network

    episode_reward.append(total_reward)
    episode_loss.append(np.mean(total_loss))
    print(f"Episode: {episode + 1}, Memory: {len(memory)}, Train Loss: {np.mean(total_loss):.4f}, Train Reward: {total_reward:.4f}, Total Profit: {info['Total profit']}")

episodes = list(range(1, num_episodes + 1))
plt.figure(figsize=(14, 5))
plt.plot(episodes, episode_loss, linestyle='-')
plt.xlabel('Episodes', size=16)
plt.ylabel('Loss', size = 16)
plt.grid(True)
plt.xticks(range(0, num_episodes + 1, 10))
plt.show()

episodes = list(range(1, num_episodes + 1))
plt.figure(figsize=(14, 5))
plt.plot(episodes, episode_reward, linestyle='-')
plt.xlabel('Episodes', size=16)
plt.ylabel('Total Reward', size = 16)
plt.grid(True)
plt.xticks(range(0, num_episodes + 1, 10))
plt.axhline(0, color='red', alpha=0.4)
plt.show()

# Set the policy network to evaluation mode
lstm_policy.eval()  # Using policy network for evaluation
env_test = StockTradingEnv(df_test_scaled, window=window_size)
state = env_test.reset()
total_reward = 0
done = False

while not done:
    action = agent.act(state, test=True)  # Only take the optimal action during testing
    next_state, reward, done, info = env_test.step(action)
    total_reward += reward
    state = next_state

print(f"Test reward: {total_reward:.4f}, Total profit: {info['Total profit']}")